---
layout: post
title: "【机器学习笔记】手撕CART"
# subtitle: '无监督元学习'
author: "iYinST"
header-style: text
# header-img: "img/post-bg-alibaba.jpg"
tags:
  - 机器学习笔记
  - 决策树
  - CART
  - 手撕代码
---

CART是分类和回归树，其原理不再赘述，本文给出一个手写的CART的思路和代码。

```python
import numpy as np
from utils import se_loss, gini_loss,time_count


class Node:
    def __init__(self, value=None, left=None, right=None, instances_index=None):
        self.value = value
        self.left = left
        self.right = right
        self.instances_index = instances_index
        self.split_feature = None
        self.split_point = None


class CART:
    def __init__(self, objective='regression', max_depth=10, min_samples_leaf=2, min_impurity_decrease=0.):
        self.objective = objective
        if self.objective == 'regression':
            self.loss = se_loss
            self.leaf_weight = np.mean
        elif self.objective == 'classification':
            self.loss = gini_loss
            self.leaf_weight = lambda y: np.argmax(np.bincount(y))

        self.min_impurity_decrease = min_impurity_decrease
        self.max_depth = max_depth
        self.root = Node()
        self.min_samples_leaf = min_samples_leaf
        self.depth = 1

    # @time_count
    def fit(self, X, y):
        self.root.instances_index = list(range(X.shape[0]))
        self._generate_node(self.root, X, y, self.depth)

    def _generate_node(self, root: Node, X: np.array, y: np.array, depth: int):

        # 大于最大深度剪枝
        self.depth = max(depth, self.depth)
        if depth >= self.max_depth:
            root.value = self.leaf_weight(y[root.instances_index])
            return

        split_feature, split_point = -1, -1
        min_loss = self.loss(y[root.instances_index])

        # 寻找分裂点
        for feature_index in range(X.shape[1]):
            split_candidate = sorted(np.unique(X[root.instances_index, feature_index]))
            for candidate in split_candidate:
                left = [i for i in root.instances_index if X[i, feature_index] <= candidate]
                right = [i for i in root.instances_index if X[i, feature_index] > candidate]

                # 小于最小样本数剪枝
                if len(left) < self.min_samples_leaf or len(right) < self.min_samples_leaf:
                    continue

                # 计算分裂后的loss
                split_loss = (len(left) * self.loss(y[left]) + len(right) * self.loss(y[right])) / len(root.instances_index)

                # 更新loss
                if split_loss < min_loss and self.loss(y[root.instances_index]) - split_loss > self.min_impurity_decrease:
                    min_loss = split_loss
                    split_feature = feature_index
                    split_point = candidate

        if split_point == -1:
            # 不分裂
            root.value = self.leaf_weight(y[root.instances_index])
        else:
            # 分裂
            root.split_point = split_point
            root.split_feature = split_feature
            root.left = Node()
            root.right = Node()

            root.left.instances_index = [i for i in root.instances_index if X[i][split_feature] <= split_point]
            root.right.instances_index = [i for i in root.instances_index if X[i][split_feature] > split_point]
            root.instances_index = None

            self._generate_node(root.left, X, y, depth + 1)
            self._generate_node(root.right, X, y, depth + 1)

    def predict(self, X):
        result = np.zeros([len(X)])
        for item, x in enumerate(X):
            root = self.root
            while root.value is None:
                if x[root.split_feature] <= root.split_point:
                    root = root.left
                else:
                    root = root.right
            result[item] = root.value
        return result
```

编写CART的思路如下。

首先考虑CART的节点需要存储哪些信息。对于叶节点，需要存储输出值；对于非叶节点，需要存储分裂的特征、分裂的特征值和左右子树。于是我们定义节点如下

```python
class Node:
    def __init__(self, value=None, left=None, right=None, instances_index=None):
        self.value = value
        self.left = left
        self.right = right
        self.instances_index = instances_index
        self.split_feature = None
        self.split_point = None
```

节点类中的left和right为节点的左右节点，value为叶节点的输出值，split_feature为分裂的特征，split_point为分裂的特征值，instances_index为中间变量，用来存储该节点要处理的样本。

在定义号节点类型后考虑CART树有哪些参数。CART树可以做分类或者回归，需要定义objective；为了防止过拟合发生，定义max_depth、min_samples_leaf和min_impurity_decrease。CART的loss对于分类和回归是不同的，对于分类任务，CART通常采用GINI作为loss，对于回归任务通常选用MSE作为loss。另外分类和回归任务还导致叶节点输出值的计算方法的差异，分类任务的叶节点输出值为分到该节点样本最多的类别，而回归任务的叶节点输出值为分到该节点样本的平均值。CART树的初始化如下。

```python
class CART:
    def __init__(self, objective='regression', max_depth=10, min_samples_leaf=2, min_impurity_decrease=0.):
        self.objective = objective
        if self.objective == 'regression':
            self.loss = se_loss
            self.leaf_weight = np.mean
        elif self.objective == 'classification':
            self.loss = gini_loss
            self.leaf_weight = lambda y: np.argmax(np.bincount(y))

        self.min_impurity_decrease = min_impurity_decrease
        self.max_depth = max_depth
        self.root = Node()
        self.min_samples_leaf = min_samples_leaf
        self.depth = 1

```

se_loss和gini_loss的计算如下。

```python
def se_loss(target: np.array, pred = None):
    if pred:
        return mean_squared_error(target,pred)
    else:
        return np.var(target) * len(target)

def gini_loss(y: np.array):
    gini = 0
    for target in np.unique(y):
        gini += np.sum(y == target) ** 2
    return 1 - gini / len(y) ** 2
```

接下来编写CART树预测的代码。对于每一个要测试的样本，从根节点出发对比样本特征值与节点分裂点选择左右子树，直到到达叶子节点，叶子节点的输出值为样本的预测值。预测部分代码如下。

```python
def predict(self, X):
        result = np.zeros([len(X)])
        for item, x in enumerate(X):
            root = self.root
            while root.value is None:
                if x[root.split_feature] <= root.split_point:
                    root = root.left
                else:
                    root = root.right
            result[item] = root.value
        return result
```

最难处理的部分是CART的训练部分。代码如下。

```python
    def fit(self, X, y):
        self.root.instances_index = list(range(X.shape[0]))
        self._generate_node(self.root, X, y, self.depth)

    def _generate_node(self, root: Node, X: np.array, y: np.array, depth: int):

        # 大于最大深度剪枝
        self.depth = max(depth, self.depth)
        if depth >= self.max_depth:
            root.value = self.leaf_weight(y[root.instances_index])
            return

        split_feature, split_point = -1, -1
        min_loss = self.loss(y[root.instances_index])

        # 寻找分裂点
        for feature_index in range(X.shape[1]):
            split_candidate = sorted(np.unique(X[root.instances_index, feature_index]))
            for candidate in split_candidate:
                left = [i for i in root.instances_index if X[i, feature_index] <= candidate]
                right = [i for i in root.instances_index if X[i, feature_index] > candidate]

                # 小于最小样本数剪枝
                if len(left) < self.min_samples_leaf or len(right) < self.min_samples_leaf:
                    continue

                # 计算分裂后的loss
                split_loss = (len(left) * self.loss(y[left]) + len(right) * self.loss(y[right])) / len(root.instances_index)

                # 更新loss
                if split_loss < min_loss and self.loss(y) - split_point > self.min_impurity_decrease:
                    min_loss = split_loss
                    split_feature = feature_index
                    split_point = candidate

        if split_point == -1:
            # 不分裂
            root.value = self.leaf_weight(y[root.instances_index])
        else:
            # 分裂
            root.split_point = split_point
            root.split_feature = split_feature
            root.left = Node()
            root.right = Node()

            root.left.instances_index = [i for i in root.instances_index if X[i][split_feature] <= split_point]
            root.right.instances_index = [i for i in root.instances_index if X[i][split_feature] > split_point]
            root.instances_index = None

            self._generate_node(root.left, X, y, depth + 1)
            self._generate_node(root.right, X, y, depth + 1)
```

从训练和预测过程可以发现，CART对于分类和回归任务的算法是一样的，只是在损失函数和叶节点输出值生成上有所差异。

最后，在波士顿房价和鸢尾花数据集上测试我们的代码。

```python
from sklearn import datasets  # 导入库
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import f1_score, precision_score, recall_score
from CART import CART
import numpy as np
from sklearn import tree
import time

# 回归
boston = datasets.load_boston()  # 导入波士顿房价数据
X, y = boston.data, boston.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state= 32)
max_depth = 9

clf = tree.DecisionTreeRegressor(max_depth=max_depth)
t = time.time()
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(mse(y_pred, y_test), time.time() - t)

cart = CART(objective='regression',max_depth=max_depth)
t = time.time()
cart.fit(X_train, y_train)
y_pred = cart.predict(X_test)
print(mse(y_pred, y_test), time.time() - t)

# 分类
iris = datasets.load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=32)
max_depth = 10

clf = tree.DecisionTreeClassifier(max_depth=max_depth)
t = time.time()
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(precision_score(y_pred, y_test, average='micro'), recall_score(y_pred, y_test, average='micro'),
      f1_score(y_pred, y_test, average='micro'), time.time() - t)

cart = CART(objective='classification', max_depth=max_depth)
t = time.time()
cart.fit(X_train, y_train)
y_pred = cart.predict(X_test)
print(precision_score(y_pred, y_test, average='micro'), recall_score(y_pred, y_test, average='micro'),
      f1_score(y_pred, y_test, average='micro'), time.time() - t)

```

输出结果。

```o
24.143986909321487 0.004000186920166016
17.756936648130747 2.0647075176239014
0.9666666666666667 0.9666666666666667 0.9666666666666667 0.007916927337646484
0.9666666666666667 0.9666666666666667 0.9666666666666667 0.05104231834411621
```

在回归任务上，我们手写的CART的mse低于sklearn中的DecisionTreeRegressor，但速度慢了2000倍。在分类任务上，效果低于sklearn。

我们编写的CART虽然可以达到一定的效果，但仍然存在效果差、速度慢、容易过拟合的问题，因此我们在下一篇中优化CART。