---
layout: post
title: "【机器学习笔记】手撕GBDT"
# subtitle: '无监督元学习'
author: "iYinST"
header-style: text
# header-img: "img/post-bg-alibaba.jpg"
tags:
  - 机器学习笔记
  - 梯度提升决策树
  - GBDT
  - 手撕代码
---

CART的优化还没写出来，下一次再更新。

梯度提升决策树（GBDT）的原理不再赘述，不熟悉的同学可以参看李航老师的《统计学习方法》，这里给出一个GBDT的实现。

在编码之前，我们首先要考虑三个问题：

1、每棵树拟合的目标是什么？

2、树的每个节点怎么分裂？

3、叶节点的预测值怎么计算？

想清楚这三个问题，就可以讲问题转成构建多个CART的问题，将复杂的问题简单化。从分类和回归两个任务来考虑，回归任务相对简单，当损失函数是均方误差（MSE）的时候，每棵树拟合的目标就是当前模型的残差，树的每个节点的分裂方法就是CART节点的分裂方法，叶节点的预测值是所有分到此节点的样本的均值。分类任务相对复杂，考虑损失函数是对数似然损失函数的二分类任务，模型预测的结果是对数几率，这里和逻辑回归是十分相似的，每棵树拟合的目标为真实值与预测概率的差值，树的每个节点的分裂方法就是CART节点的分裂方法，叶节点的预测值是$(y-P)/(P*(1-P))$，其中$y$为真实标签，$P$为预测概率。对上述三个问题进行总结，针对问题一，每棵树拟合的目标都是损失函数的负梯度，对于均方误差为损失函数的回归问题和对数似然损失函数的二分类问题，其负梯度都是真实值与预测值的差值；针对问题二，GBDT的每棵树都是CART回归树，与分类还是回归任务无关，因此节点分裂的方式即为CART回归树的分裂方式；针对问题三，叶节点的预测值应为使得损失函数最小的值，对于均方误差为损失函数的回归问题预测值为真实值的均值，对于对数似然损失函数的二分类问题预测值为$(y-P)/(P*(1-P))$。另外还要需要注意的点在于，GBDT里树的生成是按照(X,r)来建立的，但叶节点的预测值是根据真实标签和模型预测值计算的，也就是说决策树的分裂是按照响应值来分裂的。

解决了这三个问题，我们开始编写GBDT的代码。复用上篇写的CART代码，需要修改CART叶节点预测值的计算方法。

修改后的CART树如下。

```python
class CART:
    def __init__(self, objective='regression', max_depth=10, min_samples_leaf=1, min_impurity_decrease=0., real_label = None):
        self.objective = objective
        if self.objective == 'regression':
            self.loss = se_loss
            self.leaf_weight = lambda res,label: -np.mean(res)
        elif self.objective == 'classification':
            self.loss = se_loss
            self.leaf_weight = lambda res, label : np.sum(res) / np.sum((label - res) * (1 - label + res))


        self.min_impurity_decrease = min_impurity_decrease
        self.max_depth = max_depth
        self.root = Node()
        self.min_samples_leaf = min_samples_leaf
        self.depth = 1
        self.real_label = real_label

    # @time_count
    def fit(self, X, y):
        self.root.instances_index = list(range(X.shape[0]))
        self._generate_node(self.root, X, y, self.depth)

    def _generate_node(self, root: Node, X: np.array, y: np.array, depth: int):

        # 大于最大深度剪枝
        self.depth = max(depth, self.depth)
        if depth >= self.max_depth:
            root.value = self.leaf_weight(y[root.instances_index], self.real_label[root.instances_index])
            return

        split_feature, split_point = -1, -1
        min_loss = self.loss(y[root.instances_index])

        # 寻找分裂点
        for feature_index in range(X.shape[1]):
            split_candidate = sorted(np.unique(X[root.instances_index, feature_index]))
            for candidate in split_candidate:
                left = [i for i in root.instances_index if X[i, feature_index] <= candidate]
                right = [i for i in root.instances_index if X[i, feature_index] > candidate]

                # 小于最小样本数剪枝
                if len(left) < self.min_samples_leaf or len(right) < self.min_samples_leaf:
                    continue

                # 计算分裂后的loss
                split_loss = self.loss(y[left]) + self.loss(y[right])

                # 更新loss
                if split_loss < min_loss and self.loss(y[root.instances_index]) - split_loss > self.min_impurity_decrease:
                    min_loss = split_loss
                    split_feature = feature_index
                    split_point = candidate

        if split_point == -1:
            # 不分裂
            root.value = self.leaf_weight(y[root.instances_index], self.real_label[root.instances_index])
        else:
            # 分裂
            root.split_point = split_point
            root.split_feature = split_feature
            root.left = Node()
            root.right = Node()

            root.left.instances_index = [i for i in root.instances_index if X[i][split_feature] <= split_point]
            root.right.instances_index = [i for i in root.instances_index if X[i][split_feature] > split_point]
            root.instances_index = None

            self._generate_node(root.left, X, y, depth + 1)
            self._generate_node(root.right, X, y, depth + 1)

    def predict(self, X):
        result = np.zeros([len(X)])
        for item, x in enumerate(X):
            root = self.root
            while root.value is None:
                if x[root.split_feature] <= root.split_point:
                    root = root.left
                else:
                    root = root.right
            result[item] = root.value
        return result
```

与上一节CART相比，修改了self.leaf_weight，也就是叶节点权重的生成方式，这里只用到回归树，叶节点的生成方式为$\frac{\sum r}{\sum (y-r)(1-y+r)}$。

在有了修改的CART的后，我们构建GBDT的代码。

```python
class GBDT:
    def __init__(self, objective='regression', max_tree = 3, max_depth=5, min_samples_leaf=2, min_impurity_decrease=0.):
        self.objective = objective
        if self.objective == 'regression':
            self.loss = se_loss
            self.leaf_weight = np.mean
            self.model_init_func = np.mean
            self.response_gene = lambda pred,y: pred - y
            self.pred_func = lambda x: x
        elif self.objective == 'classification':
            self.loss = gini_loss
            self.leaf_weight = lambda y, pred: (y - pred) / (pred * (1 - pred))
            self.model_init_func = lambda y: - np.log( len(y) / np.sum(y) - 1)
            self.response_gene = lambda pred,y: y - sigmoid(pred)
            self.pred_func = lambda x: np.where(x > .5, 1, 0)

        self.model_init = None
        self.min_impurity_decrease = min_impurity_decrease
        self.max_depth = max_depth
        self.min_samples_leaf = min_samples_leaf
        self.depth = 1
        self.max_tree = max_tree
        self.tree_list = []
        self.tree_num = 0
        self.model = None

    def fit(self, X, y):
        if self.model is None:
            self.model_init = self.model_init_func(y)
            self.model = np.repeat(self.model_init, len(y))
        for tree_num in range(self.max_tree):
            # 计算响应值
            response =  self.response_gene(self.model, y)

            # 构建CART树
            new_cart = CART(objective=self.objective, max_depth=self.max_depth, real_label = y)
            new_cart.fit(X,response)
            f = new_cart.predict(X)

            # 添加到list
            self.model += f
            self.tree_list.append(new_cart)

    def predict(self,X):
        predict = self.model_init
        for tree in self.tree_list:
            predict += tree.predict(X)
        return self.pred_func(predict)
```

我们发现GBDT的代码比CART简单多了，因为GBDT只需要计算几个关键的数值，并生成若干CART就可以，这几个关键的数值有初始化预测值、CART的响应值等。我们的GBDT并没有添加学习率，相当于学习率为1的GBDT，如果设置一个小的学习率可以提高模型的准确率，但会导致模型收敛速度的下降，让我们速度本就不快的GBDT更是雪上加霜。

接下来测试我们编写GBDT的性能，测试代码如下。

```python
from sklearn import datasets  # 导入库
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import f1_score, precision_score, recall_score
from GBDT import GBDT
from sklearn import tree
import time

# 回归
boston = datasets.load_boston()  # 导入波士顿房价数据
X, y = boston.data, boston.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state= 32)
max_depth = 3

clf = tree.DecisionTreeRegressor(max_depth=max_depth)
clf = clf.fit(X_train, y_train)
t = time.time()
y_pred = clf.predict(X_test)
print(mse(y_pred, y_test), time.time() - t)

gbdt = GBDT(objective='regression',max_depth=max_depth)
t = time.time()
gbdt.fit(X_train, y_train)
y_pred = gbdt.predict(X_test)
print(mse(y_pred, y_test), time.time() - t)

# 分类
cancer = datasets.load_breast_cancer()  # 导入乳腺癌数据
X, y = cancer.data, cancer.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=32)
max_depth = 3

clf = tree.DecisionTreeClassifier(max_depth=max_depth)
t = time.time()
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(precision_score(y_pred, y_test), recall_score(y_pred, y_test),
      f1_score(y_pred, y_test), time.time() - t)

gbdt = GBDT(objective='classification',max_depth=max_depth)
t = time.time()
gbdt.fit(X_train, y_train)
y_pred = gbdt.predict(X_test)
print(precision_score(y_pred, y_test), recall_score(y_pred, y_test),
      f1_score(y_pred, y_test), time.time() - t)
```

测试结果。

```
29.96338709672509 0.0009992122650146484
26.27684558465425 4.171807050704956
0.9428571428571428 0.9295774647887324 0.9361702127659575 0.006042003631591797
0.8857142857142857 0.96875 0.9253731343283582 20.620769739151
```

可以看出，在回归任务上我们写的GBDT的均方误差优于sklearn中的GBDT的，分类任务中准确度和F1都略差于sklearn的GBDT，召回率高于sklearn的GBDT；但在速度上无论回归任务还是分类任务，都比sklearn慢3000倍以上。

我们通过观察代码，发现几件有趣的事情。

不是说GBDT里的树都是回归树吗，为什么实现上还分了regression和classification呢？这是由于对于不同的任务，最后得到结果的方式是不同的，因此CART的损失函数和叶节点预测值的计算方式是不同的，所以我们在CART里分成了regression和classification分别计算损失函数和叶节点预测值。

## 参考文献

1. Friedman J H. Greedy function approximation: a gradient boosting machine[J]. Annals of statistics, 2001: 1189-1232.
2. 深入理解GBDT二分类算法, https://zhuanlan.zhihu.com/p/89549390